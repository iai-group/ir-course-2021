# L9: Neural information retrieval

This module discusses neural ranking models for information retrieval. In contrast to traditional feature-based supervised learning, here, the representation of language is learned from raw text. We cover both early work (static word embeddings) and modern transformer-based approaches for text ranking.

## Lecture videos and slides

| **Module** | **Topic** | **Lecture material** |
| -- | -- | -- | 
| L9-1 | Neural IR | [lecture video](https://youtu.be/1TinPyQvd8c),  **TODO** [slides]() |
| L9-2 | Pretrained transformers for text ranking| [external video -- watch from 30:14 to 1:07:20](https://youtu.be/_Mmhl0egDmY?t=1814),  **TODO** [slides]() |

## Reading

  * An Introduction to Neural Information Retrieval (Mitra & Craswell, 2018) [PDF](https://www.microsoft.com/en-us/research/uploads/prod/2017/06/fntir2018-neuralir-mitra.pdf)
  * Pretrained Transformers for Text Ranking: BERT and Beyond (Lin, Nogueira & Yates, 2020) [PDF](https://arxiv.org/abs/2010.06467)


## Summary

Key concepts in this lecture:

 * **TODO**
